# -*- coding: utf-8 -*-
"""Chatbot_with_gradio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lyPAI-qsZnN4gdYlDG663H0L9TiSTXgk

Mistral with Gradio
"""

from huggingface_hub import login
login("Your_own_HuggingFace_API_KEY")

# Colab cell: install dependencies
!pip install --quiet sentence-transformers faiss-cpu transformers optimum

# Optional: peft for QAT and quantization
!pip install --quiet peft

!pip install faiss-cpu transformers sentence-transformers

!pip install transformers bitsandbytes accelerate

import json
import torch
import faiss
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer

# Colab cell: load context data
with open("/content/final.json", "r", encoding="utf-8") as f:
    raw_data = json.load(f)

context_data = []
for section in raw_data.values():
    entries = section.get("entries", [])
    for entry in entries:
        text = entry.get("text")
        if text:
            context_data.append(text)

print(f"✅ Loaded {len(context_data)} context entries.")
print("Preview:", context_data[:2])

embedding_model = SentenceTransformer("all-mpnet-base-v2")
context_embeddings = embedding_model.encode(context_data, convert_to_tensor=True, show_progress_bar=True)

dimension = context_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(context_embeddings.cpu().numpy())
print("✅ FAISS index built.")

# # Step 2: Build FAISS index
# dimension = context_embeddings.shape[1]
# index = faiss.IndexFlatL2(dimension)
# index.add(context_embeddings.cpu().numpy())

# Step 3: Load Mistral model
model_id = "mistralai/Mistral-7B-Instruct-v0.1"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True
)
model.eval()

def build_prompt(question, context):
    return f"""You are a helpful assistant for college website users. Use the context below to answer accurately and concisely. If you don't know the answer, just say that you don't know, don't try to make up an answer.
    Ask followup question for clarity of the question,Try to add emojies in answer to make the conversation interacive.

Context:
{context}

Question:
{question}
"""

def generate_answer(prompt, max_new_tokens=128):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            top_k=50,
            top_p=0.95,
            do_sample=True
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def retrieve_context(query, k=2):
    query_embedding = embedding_model.encode(query, convert_to_tensor=True).cpu().numpy()
    D, I = index.search(query_embedding.reshape(1, -1), k)
    retrieved_contexts = [context_data[i] for i in I[0]]
    return "\n---\n".join(retrieved_contexts)

def chatbot_response(query):
    relevant_context = retrieve_context(query, k=3)
    prompt = build_prompt(query, relevant_context)
    answer = generate_answer(prompt)
    return answer

# Test with some example queries
test_questions = [
    "How many techers are ther in the college?"]

for q in test_questions:
    print(f"\nUser: {q}")
    print("Bot:", chatbot_response(q))

# Install Gradio (if not already)
!pip -q install gradio

import gradio as gr

# Optional: clean wrapper to return only the model's final answer
def ask_bot(message, history):
    # message: latest user input
    # history: list of [user, bot] turns (you can use it if you want)
    # Use your existing pipeline
    answer = chatbot_response(message)
    # Enforce your instruction to thank the user in every answer if needed,
    # but your prompt already covers it. Here we just return the model output.
    return answer

# Build a ChatInterface
chat_ui = gr.ChatInterface(
    fn=ask_bot,
    title="College Chatbot",
    description="Ask questions about the college. Answers are generated from your RAG context and Mistral-7B.",
    examples=[
        "What is the name of the director of the college",
        "What are the admission requirements",
        "What courses are offered"
    ],
    theme="soft",
)

# Launch in Colab. share=True gives a public link in Colab.
chat_ui.launch(share=True)